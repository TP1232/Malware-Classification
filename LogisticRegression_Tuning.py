import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import time
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression


df = pd.read_csv("/archive/Dataset/Dataset/train/LargeTrain.csv")

X = df.iloc[:,:-1]

Y = df.iloc[:,-1]

scaler = MinMaxScaler()
scaler.fit(X)
normalized_x = scaler.transform(X)
normalized_y = Y

X_train, X_test, Y_train, Y_test = train_test_split(normalized_x, normalized_y, shuffle = True, test_size=0.2)

# =============================================================================
# Logistic Tuning
# =============================================================================

model = LogisticRegression()

param_grid = [{'penalty': ['l1'],
                'solver': ['liblinear', 'saga'],
                'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},
              {'penalty': ['l2'],
                'solver': ['newton-cg', 'lbfgs', 'sag'],
                'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
                'max_iter': [100, 200, 300]}]


print("Hyper Parameter Tuning Results\n")

# Finding optimum parameters through GridSearchCV
grid = GridSearchCV(estimator=model, param_grid = param_grid,
                    cv = 5)
grid.fit(X_train, Y_train)

print("\n")
print("Results returned by GridSearchCV\n")
print("Best estimator: ", grid.best_estimator_)
print("\n")
print("Best score: ", grid.best_score_)
print("\n")
print("Best parameters found: ", grid.best_params_)

